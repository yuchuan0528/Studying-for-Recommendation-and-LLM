# 背景

* 传统DLRM采用**级联**策略，召回-粗排-精排。每个独立的排序器的效果决定了最终的结果上限，即使进行排序器间的交互。
* GR:基于**生成式的召回**方法。但这类方法只能用于召回，准确性无法与多级联的模型相比。

!!! note 本文贡献
    1. **Encoder-Decoder架构**：基于MoE模型，提升模型的参数量
    2. **提出基于会话列表生成的方法**：不同于point-by-point generation（需要制定策略保证多样性和连贯性），session-wise自动学习会话内整体物品的内容和顺序
    3. **使用DPO进行偏好学习**。不随机抽取负样本，而是从束搜索结果中创建self-head rejected samples。（关于抽样，提出了迭代偏好对齐策略，用预训练的奖励模型提出的得分对采样结果进行排序，确定最优/差rejected samples）

* 关于大模型的偏好对齐：
  * 1. RLHF：LLM通常使用RLHF，通过强化学习，将LLM结果与人类价值观进行对齐。但RLHF效率低（先训练奖励模型RM，然后用RM指导大模型）
  * 2. DPO：**跳过了训练奖励模型和强化学习**这两个步骤，而是设计了一个巧妙的损失函数，**直接在偏好数据上对LLM进行微调**。
   
# Tokennizer
通过使用精简和固定的词汇表，将物品token为coarse-to-fine的语义ID。将协作信号与多模态特征整合在一起，然后利用 RQ-Kmeans（Luo 等人，2024 年）生成更高质量的分层语义 ID。

* **对齐的协作感知多模态表示**
  * 1. 多模态表示：将标题、标签、视频等多个那栋，使用模型进行处理，然后使用基于TRansformer的模型，对每个样本生成**4个向量**（coarse-to-fine的语义ID），作为该物品的表示。
  * 2. 相似物品对：使用U2I或者I2I召回，基于高相似度，得到物品对。
  * 3. 训练目标：item to item的对比损失与下一个目标预测损失，

* **Tokenization**
  * 1. 使用RQ-Kmeans进行分词。采用残差两话技术，从粗到细生成语义ID。直接对残差进行K-means聚类构建编码本。
  
## Encoder
### 多尺度的特征工程
包含多个模式：用户静态途径、短期途径、正反馈途径和终身途径。
* **User static Pathway**:
    包含用户核心特征，例如id， age等，然后转化为隐层维度
* **Short-term Pathway**：
    包含用户最近交互过的L（如20）个物品：物品标识符（可以像tokenizer一节中这样表示），作者标识符，标签等。同样
* **Positive-feedback Pathway**：
    一串高参与互动的基础上进行的（如256个物品）
* **Lifelong Pathway**：
    处理超长的用户行为序列。采用了两个阶段对这一超长序列分层压缩。

***
* 输入：$n$长的用户历史行为序列（例如视频id）
* 输出：$s$长的输出预测结果序列

对于每个item：$v_i$，用多模态嵌入$e_i$进行表示。采用**多级平衡量化**机制，用**残差k-means算法**对$e_i$进行转换：
    1. $level = 1$时：残差即是本身。$r_i^1 = e_i$