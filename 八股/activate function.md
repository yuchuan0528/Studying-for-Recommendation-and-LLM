这是一个非常基础且核心的深度学习问题。这三个激活函数在神经网络中扮演着截然不同的关键角色。

我将为您详细、正式地介绍这三个函数。

---

### 1. Sigmoid 激活函数

Sigmoid函数（也称为Logistic函数）是早期神经网络（尤其是浅层网络）中最常用的激活函数之一。

#### 1.1 原理与定义

Sigmoid函数的核心功能是将一个实数输入“压缩”到 (0, 1) 的开区间内。

它的形状呈“S”型，这个特性使其在历史上被用来模拟生物神经元的“激发率”（Firing Rate）——从0（完全不激发）到1（完全饱和激发）。

在现代应用中，它最主要的角色是作为二元分类（Binary Classification）问题中**输出层**的激活函数，用于将模型的原始输出（Logits）转换为一个**概率值**。

#### 1.2 数学公式

Sigmoid函数，通常表示为 $\sigma(x)$，其数学定义如下：

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

其导数（在反向传播中至关重要）具有一个非常优雅的形式：

$$
\sigma'(x) = \sigma(x) (1 - \sigma(x))
$$

#### 1.3 优缺点

**优点：**

1.  **平滑可导：** 函数在定义域内处处平滑且可导，非常适合用于基于梯度的优化算法（如反向传播）。
2.  **输出为概率：** 其 (0, 1) 的输出范围天然契合了概率的定义，使其成为二元分类输出层的不二之选。
3.  **数据归一化：** 它可以将任意范围的输入压缩到 (0, 1) 之间，起到一种“归一化”或“压缩”数据的作用，常用于循环神经网络（RNN）中的门控单元（Gating Mechanism）。

**缺点：**

1.  **梯度消失（Vanishing Gradients）：** 这是Sigmoid最致命的缺点。
    * 观察其导数 $\sigma'(x)$，当输入 $x$ 非常大（例如 $x > 5$）或非常小（例如 $x < -5$）时，$\sigma'(x)$ 的值会**无限趋近于0**。
    * 在深度神经网络中，反向传播需要将梯度逐层相乘。如果多层都使用Sigmoid，这些接近0的梯度相乘，会导致传播到浅层网络的梯度变得“微乎其微”，使得浅层网络的参数几乎无法更新。
2.  **输出非零中心（Not Zero-Centered）：**
    * Sigmoid的输出恒为正数 (0, 1)。
    * 这会导致在反向传播中，下一层接收到的梯度**要么全部为正，要么全部为负**（取决于上游梯度）。
    * 这会导致权重更新时出现效率低下的“Z字形”更新（Zigzagging Updates），拖慢收敛速度。
3.  **计算成本：** 包含指数运算（$e^{-x}$），相对于ReLU等函数，计算成本更高。

---

### 2. ReLU (Rectified Linear Unit) 修正线性单元

ReLU是目前深度学习中（尤其是卷积神经网络CNN和多层感知机MLP中）**最常用**的**隐藏层**激活函数。

#### 2.1 原理与定义

ReLU的原理极其简单：它是一个分段线性函数。

* 如果输入大于0，则原样输出。
* 如果输入小于或等于0，则输出0。

这个看似简单的“阈值”操作，却解决了Sigmoid的诸多痛点，极大地促进了深度网络的发展。

#### 2.2 数学公式

ReLU函数的数学定义如下：

$$
\text{ReLU}(x) = \max(0, x)
$$

也可以表示为分段函数：

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}
$$

其导数（在 $x=0$ 处次导数为0或1均可）为：

$$
f'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}
$$

#### 2.3 优缺点

**优点：**

1.  **解决梯度消失（正区间）：** 当 $x > 0$ 时，其导数恒为1。这意味着在正激活区域，梯度可以以恒定的大小流过，不会在深层网络中衰减，极大地缓解了梯度消失问题。
2.  **计算效率极高：** 其计算只涉及一个 `max(0, x)` 操作，不含指数等复杂运算，计算速度非常快。
3.  **引入稀疏性（Sparsity）：** 当 $x \le 0$ 时，神经元输出为0，导数也为0。这使得一部分神经元被“关闭”，从而使网络具有“稀疏激活”的特性。稀疏性可以提高计算效率，并可能有助于减少过拟合。

**缺点：**

1.  **Dying ReLU（神经元“死亡”）：** 这是ReLU最主要的问题。
    * 如果一个神经元在训练中（例如由于一次不良的梯度更新或较大的负偏置）其输入恒为负，那么它的输出将恒为0。
    * 由于 $x \le 0$ 时梯度为0，这个神经元在反向传播中将**永远无法接收到梯度**。
    * 它将停止学习和更新，如同“死亡”一般，对整个网络不再有贡献。
2.  **输出非零中心：** 与Sigmoid一样，ReLU的输出 $\ge 0$，存在非零中心问题，可能导致收敛变慢。
3.  **在 $x=0$ 处不可导：** 这是一个理论上的数学瑕疵，但在工程实践中，我们可以在该点任意取0或1作为次导数，并不影响实际训练。

---

### 3. Softmax 激活函数

Softmax函数与Sigmoid和ReLU**根本不同**。它**不是**一个逐点（Point-wise）的激活函数，而是应用于一个**向量（Vector）**。

它几乎**只被用作多类分类（Multi-Class Classification）问题**的**输出层**。

#### 3.1 原理与定义

Softmax的功能是：接收一个包含 $K$ 个任意实数（Logits）的向量，并将其转换为一个包含 $K$ 个实数的**概率分布**。

“概率分布”意味着：
1.  向量中的每个元素都在 (0, 1) 范围内。
2.  向量中所有元素的**总和恰好为 1**。

它通过指数函数来“放大”输入值之间的差异，然后通过除以总和来进行归一化。

#### 3.2 数学公式

给定一个 $K$ 维的输入向量 $Z = (z_1, z_2, \ldots, z_K)$，Softmax函数对第 $i$ 个元素 $z_i$ 的计算公式为：

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, \ldots, K
$$

* **分子 $e^{z_i}$**：指数函数确保所有输出值均为正数，并且能拉大原始分数的差距（“赢家通吃”）。
* **分母 $\sum_{j=1}^{K} e^{z_j}$**：这是所有元素的指数和，作为归一化因子，确保所有输出值的总和为1。

**重要提示：数值稳定性（Numerical Stability）**
在实际计算中，如果 $z_i$ 很大（例如1000），$e^{1000}$ 会导致数值溢出（Overflow）。
工程上会使用一个“稳定Softmax”技巧：将输入向量 $Z$ 的所有元素同时减去 $Z$ 中的最大值 $\max(Z)$：

$$
\text{Softmax}(z_i) = \frac{e^{z_i - \max(Z)}}{\sum_{j=1}^{K} e^{z_j - \max(Z)}}
$$

这在数学上与原始公式等价，但避免了计算超大数值的指数。

#### 3.3 优缺点

**优点：**

1.  **概率分布输出：** 完美适用于多类分类问题，输出可以被直观地解释为“样本属于第 $i$ 类的概率”。
2.  **完美配合交叉熵：** 其输出的概率分布是计算“分类交叉熵损失”（Categorical Cross-Entropy Loss）的理想输入。
3.  **可微分：** 整个函数是可微的，允许梯度从损失函数反向传播回网络。

**缺点：**

1.  **非逐点激活：** 它不用于隐藏层，因为它会“压制”特征（强迫它们和为1），破坏特征的独立表达。
2.  **计算成本：** 涉及多次指数运算和一次除法，计算成本高于ReLU。
3.  **对输入幅度敏感：** 虽然有数值稳定技巧，但其“赢家通吃”的特性可能导致梯度在某些情况下（例如所有Logits都差不多时）变得很小。

### 总结对比

| 特性 | Sigmoid | ReLU | Softmax |
| :--- | :--- | :--- | :--- |
| **数学公式** | $\frac{1}{1 + e^{-x}}$ | $\max(0, x)$ | $\frac{e^{z_i}}{\sum e^{z_j}}$ |
| **应用类型** | 逐点（Point-wise） | 逐点（Point-wise） | 向量（Vector-wise） |
| **典型应用** | 二元分类（输出层）、门控（RNN） | 深度网络的**隐藏层** | 多类分类（**输出层**） |
| **输出范围** | (0, 1) | [0, $+\infty$) | (0, 1)，且总和为1 |
| **核心优点** | 输出为概率，平滑可导 | 计算快，解决梯度消失（正区） | 输出为概率分布 |
| **核心缺点** | **梯度消失**（饱和区） | **Dying ReLU**（负区） | 只用于输出层，计算稍贵 |
| **零中心** | 否 | 否 | 否 |

希望这个正式的介绍对您有所帮助。

您是否想了解ReLU的一些高级变体，比如 **Leaky ReLU**、**ELU** 或 **GeLU**（GPT等模型中使用的）？