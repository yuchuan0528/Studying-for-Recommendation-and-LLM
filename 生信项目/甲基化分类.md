# 特征提取
两个**非对称**特征提取网络：MLP

* 第一个**更深、更宽**：1w -> 2048 -> 1024 > 512 > 256，从这个高维空间中逐步提取和抽象出复杂的非线性特征。**参数更多**，通常具有更高的**过拟合**风险，使用**更高的 Dropout**率。（p = 0.5）
* 第二个**相对较浅**，防止过拟合，提高计算效率。3k -> 512 -> 256。dropout p = 0.2

<font color=red> 什么是过拟合/欠拟合 </font>

**欠拟合**： 模型未能充分捕捉训练数据中的基本结构和模式。本质：模型**复杂度过低**

**过拟合** ：模型在训练集上表现优异，但在未见过的验证集，上性能显著下降。模型**失去了泛化能力**。本质：模型复杂度过高，

**判断方法**：绘制loss曲线。欠拟合：训练集、测试集loss都很高，无法收敛。过拟合：训练集持续下降，测试集下降后反升。

**应对方法**：
    * **欠拟合**：增加网络深度、复杂度，降低dropout率，增加epoch
    * **过拟合**：降低网络复杂度，增加dropout，加入BN, 在验证集中加入早停机制

<font color=red> 关于GELU激活函数 </font>
GELU引入一种随机正则化的思想来对神经元的激活进行“门控”（gating）。不再像RELU根据输入的正负进行判断，而是一种**平滑的概率**。

GELU函数处处平滑可导，使得**优化过程更加稳定**（避免=0时的不可导）。并且允许负值，是非线性函数，保留了更多信息，**增加了模型的表达能力**。同时，在输入为负时依然导数不为零，**解决了RELU的死亡神经元问题**，（累积分布函数难以计算，实际中使用tanh近似函数） 

# 特征融合
输入数据是经过特征提取得到的二维数据：(batch_size, feature_dim)。**我们将每个特征看作一个序列中的不同元素**（将D个特征看作视为一个长度为 D 的序列），为每个特征学习一个嵌入表示。因此，经过一个线性层`nn.Linear(D, D*E)`,然后经过`view`得到了**想要的三维数据：(batch_size, feature_dim, Embedding_dim)**

<font color=red>LN与BN</font>

# 对比学习
对于每个分支，设置了专门的投影头，将特征映射到128维中，进行对比学习。

**投影头的作用**：投影头 $g$ 的作用是充当“缓冲区”，保护编码器 $f$ 提取的特征 $h$ 免受对比损失的“信息破坏”。对比损失会给编码器 $f$ 施加压力，迫使其“丢弃”不是类别本身信息但是对下游分析非常有用的细粒度的特征。使用投影头，可以**让投影头负责丢弃无关特征**，**编码器继续保留丰富特征**

**损失计算**：对于第$i$个样本，计算时要找到所有其他的正样本 $P(i)$，然后计算有监督的对比损失（分母是与其他所有样本的余弦相似度，分子是与正样本的余弦相似度） --> 因此需要大批次或类别感知的采样（若没有其他正样本，损失为0）

$$\mathcal{L}_i^{\text{sup}} = \frac{-1}{|P(i)|} \sum_{p \in P(i)} \log \left( \frac{\exp(\text{sim}(z_i, z_p) / \tau)}{\sum_{a=1, a \ne i}^{2N} \exp(\text{sim}(z_i, z_a) / \tau)} \right)$$

温度系数$\tau$决定了 Softmax 函数输出的概率分布是“尖锐的”还是“平滑的”,tau越小，输出约尖锐，对于困难负样本（和正样本相似，实则为负样本）也给予很高的权重。

**计算改进-memory bank**问题：需要用很大的batch size解决负样本太少的问题。Memory bank: 为数据集中的每一个样本（Instance）都开辟一个存储槽位，用来存储它最新的特征向量，负样本直接从Memory bank中采样。

**采用类别感知采样，避免没有正样本**：在创建dataloader中，设置每个batch内有k个类别，每个类别有m个实例。（缺点：造成稀有类型过度采样）

**自监督的对比损失**：不再是有多个正样本
$$\text{Probability} = \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1, k \ne i}^{2N} \exp(\text{sim}(z_i, z_k) / \tau)}$$
# 类原型分类器
**衡量样本 x 到每一个“类原型子空间”的【负投影误差】**

初始化时，保构成每个“平面”的 D 个基向量是**标准正交**的。

为了保证标准正交，我们在损失中添加正则化项：$L_{\text{reg}}$，惩罚 $U @ U^T$ 与 $I$ 之间的差异（也用mse_loss实现）

对于一个向量在空间中的投影，可以看作是基向量的线性表示：（目的：求解系数）
$$x_{\text{proj}} = c_1 u_1 + c_2 u_2 + ... + c_D u_D$$

由于基向量是标准正交的，**点积即为线性组合的系数**：$c_i = x \cdot u_i$

因此，可以用矩阵一次性计算所有系数：$x @ U^T$，再乘以基向量就是投影：（运用乘法的结合律）$x_{\text{proj}} = x @ (U^T @ U)$。因此，可以得到**投影矩阵$p = (U^T @ U)$
**
对于任意输入样本，基于投影矩阵可以计算得到投影长度$x_proj = x @ P$，然后计算原向量与误差向量之间的平方欧几里得距离（**投影误差**，与MSE计算过程相同）`F.mse_loss(x_proj, x, reduction='none').sum(-1)` 按照最后一个维度对特征维度求和，即为投影误差。

# 损失
cross_entropy