# 特征提取
两个**非对称**特征提取网络：MLP

* 第一个**更深、更宽**：1w -> 2048 -> 1024 > 512 > 256，从这个高维空间中逐步提取和抽象出复杂的非线性特征。**参数更多**，通常具有更高的**过拟合**风险，使用**更高的 Dropout**率。（p = 0.5）
* 第二个**相对较浅**，防止过拟合，提高计算效率。3k -> 512 -> 256。dropout p = 0.2

<font color=red> 什么是过拟合/欠拟合 </font>

**欠拟合**： 模型未能充分捕捉训练数据中的基本结构和模式。本质：模型**复杂度过低**

**过拟合** ：模型在训练集上表现优异，但在未见过的验证集，上性能显著下降。模型**失去了泛化能力**。本质：模型复杂度过高，

**判断方法**：绘制loss曲线。欠拟合：训练集、测试集loss都很高，无法收敛。过拟合：训练集持续下降，测试集下降后反升。

**应对方法**：
    * **欠拟合**：增加网络深度、复杂度，降低dropout率，增加epoch
    * **过拟合**：降低网络复杂度，增加dropout，加入BN, 在验证集中加入早停机制

<font color=red> 关于GELU激活函数 </font>
GELU引入一种随机正则化的思想来对神经元的激活进行“门控”（gating）。不再像RELU根据输入的正负进行判断，而是一种**平滑的概率**。

GELU函数处处平滑可导，使得**优化过程更加稳定**（避免=0时的不可导）。并且允许负值，是非线性函数，保留了更多信息，**增加了模型的表达能力**。同时，在输入为负时依然导数不为零，**解决了RELU的死亡神经元问题**，（累积分布函数难以计算，实际中使用tanh近似函数） 

# 特征融合
输入数据是经过特征提取得到的二维数据：(batch_size, feature_dim)。**我们将每个特征看作一个序列中的不同元素**（将D个特征看作视为一个长度为 D 的序列），为每个特征学习一个嵌入表示。因此，经过一个线性层`nn.Linear(D, D*E)`,然后经过`view`得到了**想要的三维数据：(batch_size, feature_dim, Embedding_dim)**

<font color=red>LN与BN</font>

# 对比学习
对于每个分支，设置了专门的投影头，将特征映射到128维中，进行对比学习。

**投影头的作用**：投影头 $g$ 的作用是充当“缓冲区”，保护编码器 $f$ 提取的特征 $h$ 免受对比损失的“信息破坏”。对比损失会给编码器 $f$ 施加压力，迫使其“丢弃”不是类别本身信息但是对下游分析非常有用的细粒度的特征。使用投影头，可以**让投影头负责丢弃无关特征**，**编码器继续保留丰富特征**

**损失计算**：对于第$i$个样本，计算时要找到所有其他的正样本 $P(i)$，然后计算有监督的对比损失（分母是与其他所有样本的余弦相似度，分子是与正样本的余弦相似度） --> 因此需要大批次或类别感知的采样（若没有其他正样本，损失为0）

$$\mathcal{L}_i^{\text{sup}} = \frac{-1}{|P(i)|} \sum_{p \in P(i)} \log \left( \frac{\exp(\text{sim}(z_i, z_p) / \tau)}{\sum_{a=1, a \ne i}^{2N} \exp(\text{sim}(z_i, z_a) / \tau)} \right)$$

温度系数$\tau$决定了 Softmax 函数输出的概率分布是“尖锐的”还是“平滑的”,tau越小，输出约尖锐，对于困难负样本（和正样本相似，实则为负样本）也给予很高的权重。

**计算改进-memory bank**问题：需要用很大的batch size解决负样本太少的问题。Memory bank: 为数据集中的每一个样本（Instance）都开辟一个存储槽位，用来存储它最新的特征向量，负样本直接从Memory bank中采样。

**采用类别感知采样，避免没有正样本**：在创建dataloader中，设置每个batch内有k个类别，每个类别有m个实例。（缺点：造成稀有类型过度采样）

**自监督的对比损失**：不再是有多个正样本
$$\text{Probability} = \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1, k \ne i}^{2N} \exp(\text{sim}(z_i, z_k) / \tau)}$$
# 类原型分类器
**衡量样本 x 到每一个“类原型子空间”的【负投影误差】**

初始化时，保构成每个“平面”的 D 个基向量是**标准正交**的。

为了保证标准正交，我们在损失中添加正则化项：$L_{\text{reg}}$，惩罚 $U @ U^T$ 与 $I$ 之间的差异（也用mse_loss实现）

对于一个向量在空间中的投影，可以看作是基向量的线性表示：（目的：求解系数）
$$x_{\text{proj}} = c_1 u_1 + c_2 u_2 + ... + c_D u_D$$

由于基向量是标准正交的，**点积即为线性组合的系数**：$c_i = x \cdot u_i$

因此，可以用矩阵一次性计算所有系数：$x @ U^T$，再乘以基向量就是投影：（运用乘法的结合律）$x_{\text{proj}} = x @ (U^T @ U)$。因此，可以得到**投影矩阵$p = (U^T @ U)$
**
对于任意输入样本，基于投影矩阵可以计算得到投影长度$x_proj = x @ P$，然后计算原向量与误差向量之间的平方欧几里得距离（**投影误差**，与MSE计算过程相同）`F.mse_loss(x_proj, x, reduction='none').sum(-1)` 按照最后一个维度对特征维度求和，即为投影误差。

# 损失
**cross_entropy**：
在多分类中，CCE 的通用数学公式是计算 y 中每个元素与 p 中对应元素的乘积的负对数之和
$$\mathcal{L}_{CCE} = - \sum_{c=1}^{C} y_c \cdot \log(p_c)$$

由于标签进行了one-hot编码，非真实标签处$y_c = 0 $，因此多分类交叉熵（CCE）的计算可以简化为
$\mathcal{L} = - \log(p_{\text{correct\_class}})$
即**正确类别预测概率的负对数**。

**Focal loss**:
思想：数据中可能存在稀有类别，但这部分的损失被主要类别（简单样本）占主导。 --> 使模型**聚焦于困难样本**

计算方法：
$$\mathcal{L}_{FL} = - \alpha_t (1 - p_t)^\gamma \cdot \log(p_t)$$
其中，$(1 - p_t)^\gamma$会**动态地缩放损失**。对于困难样本， p->0， $(1 - p_t)^\gamma$更大。当$\gamma = 0$时，损失退化为交叉熵损失

$\alpha_t$类似crossentropy的类别权重， 可以为少数类设置一个较高的 $\alpha$（例如 0.75），为多数类设置一个较低的 $\alpha$


# 评估指标

**召回率 (Recall)**： $TP / (TP + FN)$ 在所有预测为正的样本中，真正预测对了多少


**F1-Score (F1分数)**： 2 * (Precision * Recall) / (Precision + Recall)

精确率和召回率的“调和平均数”，一个综合性的好指标。
调和平均数的标准数学定义是： “数值个数 / (各数值倒数之和)“，会惩罚数据中的极端不平衡现象，因此有系数2。

**多分类f1-macro**：为每个类别计算 Precision, Recall, 和 F1-Score，然后取平均。

**kappa**:模型的准确率（$P_o$），比一个‘只会按比例瞎猜’的随机模型（$P_e$）好多少？
其中，$$\kappa = \frac{P_o - P_e}{1 - P_e}$$$P_o$ = Observed Accuracy（观察到的准确率，即模型的真实准确率）。（模型的总准确率）
$P_e$ = Expected Accuracy（期望的准确率，即“随机猜测”的准确率）。$P_e$ 是指“模型的预测分布”与“数据的真实分布”偶然达成一致的概率。
$$P_e = \sum_{c \in \text{Classes}} P(\text{True}=c) \cdot P(\text{Pred}=c)$$